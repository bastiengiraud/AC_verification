BatchNorm1d may raise error during conversion when the model is in train() mode!
Set model to eval() mode!
Running on cpu
/zhome/43/1/159866/PINN/MinMax/NN_Training_DC_Crown.py:329: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  x=torch.tensor(x.detach(), requires_grad=True).to(device)
Traceback (most recent call last):
  File "/zhome/43/1/159866/PINN/MinMax/NN_Training_DC_Crown.py", line 133, in train
    training_loss = train_epoch(network_gen, InputNN, OutputNN, typNN,optimizer,config,simulation_parameters)
  File "/zhome/43/1/159866/PINN/MinMax/NN_Training_DC_Crown.py", line 245, in train_epoch
    batch_loss.backward()
  File "/zhome/43/1/159866/miniconda3/envs/alpha-beta-crown/lib/python3.7/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/zhome/43/1/159866/miniconda3/envs/alpha-beta-crown/lib/python3.7/site-packages/torch/autograd/__init__.py", line 175, in backward
    allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass
Exception